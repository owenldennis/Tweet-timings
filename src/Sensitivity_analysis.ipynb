{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity analysis of population parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising a random population None, size 1, with betas [20.0]\n",
      "Betas for population None are [20.0]\n",
      "Initialising a random population None, size 1, with betas [20.0]\n",
      "Betas for population None are [20.0]\n",
      "Initialising a random population None, size 1, with betas [20.0]\n",
      "Betas for population None are [20.0]\n",
      "Initialising a random population None, size 1, with betas [20.0]\n",
      "Betas for population None are [20.0]\n",
      "Initialising for key Owen_noise\n",
      "Expected 25 beta values, but only 1 values passed.  Initialising all time series with beta parameter 5.0\n",
      "Initialising a random population Owen_noise, size 25, with betas [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0]\n",
      "Betas for population Owen_noise are [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0]\n",
      "Elapsed time: 0.31406402587890625\n",
      "Initialising for key Owen\n",
      "Creating lagging time series population based on single event time series\n",
      "The poisson rates for lagging population Owen are [7 2 3 4 7 5 6 6 9 3 5 9 4 7 7 9 1 6 7 1 8 3 5 7 7]\n",
      "Initialising population Owen based on a prior object with event beta [20.0]\n",
      "Mean lags for population Owen are [7 2 3 4 7 5 6 6 9 3 5 9 4 7 7 9 1 6 7 1 8 3 5 7 7]\n",
      "Now combining poisson processes Owen with Owen_noise\n",
      "Elapsed time: 0.5333554744720459\n",
      "Initialising for key Elena_noise\n",
      "Expected 25 beta values, but only 1 values passed.  Initialising all time series with beta parameter 5.0\n",
      "Initialising a random population Elena_noise, size 25, with betas [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0]\n",
      "Betas for population Elena_noise are [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0]\n",
      "Elapsed time: 0.9036777019500732\n",
      "Initialising for key Elena\n",
      "Creating lagging time series population based on single event time series\n",
      "The poisson rates for lagging population Elena are [7 6 4 4 6 6 5 8 5 4 8 4 1 2 5 5 8 6 4 3 9 9 8 4 1]\n",
      "Initialising population Elena based on a prior object with event beta [20.0]\n",
      "Mean lags for population Elena are [7 6 4 4 6 6 5 8 5 4 8 4 1 2 5 5 8 6 4 3 9 9 8 4 1]\n",
      "Now combining poisson processes Elena with Elena_noise\n",
      "Elapsed time: 1.132826566696167\n",
      "Initialising for key Luke_noise\n",
      "Expected 25 beta values, but only 1 values passed.  Initialising all time series with beta parameter 5.0\n",
      "Initialising a random population Luke_noise, size 25, with betas [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0]\n",
      "Betas for population Luke_noise are [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0]\n",
      "Elapsed time: 1.430826187133789\n",
      "Initialising for key Luke\n",
      "Creating lagging time series population based on single event time series\n",
      "The poisson rates for lagging population Luke are [6 5 2 7 8 6 1 6 8 3 4 7 9 4 6 3 9 8 8 2 3 6 4 1 4]\n",
      "Initialising population Luke based on a prior object with event beta [20.0]\n",
      "Mean lags for population Luke are [6 5 2 7 8 6 1 6 8 3 4 7 9 4 6 3 9 8 8 2 3 6 4 1 4]\n",
      "Now combining poisson processes Luke with Luke_noise\n",
      "Elapsed time: 1.6203124523162842\n",
      "Initialising for key Jill_noise\n",
      "Expected 25 beta values, but only 1 values passed.  Initialising all time series with beta parameter 5.0\n",
      "Initialising a random population Jill_noise, size 25, with betas [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0]\n",
      "Betas for population Jill_noise are [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0]\n",
      "Elapsed time: 1.963252305984497\n",
      "Initialising for key Jill\n",
      "Creating lagging time series population based on single event time series\n",
      "The poisson rates for lagging population Jill are [3 8 9 6 9 3 5 5 3 6 4 8 7 8 4 6 3 4 6 7 3 2 2 1 6]\n",
      "Initialising population Jill based on a prior object with event beta [20.0]\n",
      "Mean lags for population Jill are [3 8 9 6 9 3 5 5 3 6 4 8 7 8 4 6 3 4 6 7 3 2 2 1 6]\n",
      "Now combining poisson processes Jill with Jill_noise\n",
      "Elapsed time: 2.127565860748291\n",
      "\n",
      "\n",
      "For key Owen_noise: From 25 time series length 10000, measured population event probability = 0.18182\n",
      "Theoretical population mean is 0.18182 with std 0.0\n",
      "\n",
      "For key Owen: From 25 time series length 10000, measured population event probability = 0.22325599999999998\n",
      "Theoretical population mean is 0.22325599999999998 with std 0.0\n",
      "\n",
      "For key Elena_noise: From 25 time series length 10000, measured population event probability = 0.18115599999999998\n",
      "Theoretical population mean is 0.18115599999999998 with std 0.0\n",
      "\n",
      "For key Elena: From 25 time series length 10000, measured population event probability = 0.221808\n",
      "Theoretical population mean is 0.221808 with std 0.0\n",
      "\n",
      "For key Luke_noise: From 25 time series length 10000, measured population event probability = 0.181356\n",
      "Theoretical population mean is 0.181356 with std 0.0\n",
      "\n",
      "For key Luke: From 25 time series length 10000, measured population event probability = 0.21944\n",
      "Theoretical population mean is 0.21944 with std 0.0\n",
      "\n",
      "For key Jill_noise: From 25 time series length 10000, measured population event probability = 0.183136\n",
      "Theoretical population mean is 0.18313600000000002 with std 2.7755575615628914e-17\n",
      "\n",
      "For key Jill: From 25 time series length 10000, measured population event probability = 0.221752\n",
      "Theoretical population mean is 0.22175200000000003 with std 2.7755575615628914e-17\n",
      "\n",
      "T is 10000\n",
      "Analysis of tweet matrix 0: 100 time series length 10000\n",
      "Analysis of tweet matrix 1: 0 time series length 10000\n",
      "Running with inferred means (version 1).  Time elapsed: 0.9555308818817139\n",
      "1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,"
     ]
    }
   ],
   "source": [
    "import testing_and_analysis_functions as analysis_func\n",
    "import poisson_processes as pp\n",
    "import pointwise_correlation as pc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import Louvain\n",
    "plt.rcParams['figure.figsize']=[20,10]\n",
    "\n",
    "verbose=False\n",
    "individuals_within_a_population_have_same_incidence=True\n",
    "copy_results_out_of_temp_dir=False\n",
    "reclustering='greedy Louvain'\n",
    "\n",
    "length = 10000\n",
    "number_of_populations=4\n",
    "size=25\n",
    "lag=10\n",
    "event_prob=0.05\n",
    "noise_prob=0.2\n",
    "interpret_as_max_values=False\n",
    "\n",
    "if interpret_as_max_values:\n",
    "    metaparams=analysis_func.create_random_metaparams(length=length,number_of_populations=number_of_populations,\n",
    "                                                      max_size=size,max_lag=lag,max_event_prob=event_prob,max_noise_prob=noise_prob)\n",
    "else:\n",
    "    metaparams=analysis_func.create_fixed_metaparams(length=length,number_of_populations=number_of_populations,\n",
    "                                                     size=size,lag=lag,event_prob=event_prob,noise_prob=noise_prob)\n",
    "\n",
    "population_params=analysis_func.directly_initialise_multiple_populations(length=length,metaparams=metaparams,\n",
    "                                                       use_fixed_means=individuals_within_a_population_have_same_incidence)\n",
    "\n",
    "#Copy parameters to csv file in TEMP directory\n",
    "pd.DataFrame.from_dict(metaparams,orient='index').to_csv(\"{0}\\meta_params.csv\".format(pc.TEMP_DIR))\n",
    "df=pd.DataFrame.from_dict(population_params,orient='index')\n",
    "df['prior process beta'] = df['prior poisson process'].apply(lambda x: None if type(x)==float else x.betas)\n",
    "df['mean lag list']=df['lag parameters'].apply(lambda x: None if type(x)==float else x['mus'])\n",
    "df=df.drop(['prior poisson process','lag parameters'],axis=1)\n",
    "df.to_csv(\"{0}\\population_parameters.csv\".format(pc.TEMP_DIR))\n",
    "    \n",
    "    \n",
    "#Intialise time series objects within their populations    \n",
    "mpp = pp.mixed_poisson_populations(length,population_params,verbose=verbose)\n",
    "mpp.display(stats=True)\n",
    "\n",
    "# mix populations and extract the full array of time_series_objects\n",
    "ts_obj1=mpp.randomly_mix_populations(list(metaparams.keys()))\n",
    "number=len(ts_obj1)   # total number of all time series in all populations        \n",
    "ts_obj2=[]\n",
    "\n",
    "# compare v1 and v2 sigma measures and store correlations\n",
    "td,df=analysis_func.compare_inferred_and_known_means(xs=[length],number=number,ts_matrices=[ts_obj1,ts_obj2],\n",
    "                                                    reclustering='greedy Louvain')\n",
    "plt.show()\n",
    "display(df)\n",
    "\n",
    "# copy results out of pc.TEMP_DIR directory into given directory\n",
    "if copy_results_out_of_temp_dir:\n",
    "    analysis_func.copy_from_temp(dest_root_dir=\"{0}/Multiple_population_correlations\".format(pc.RESULTS_DIR))\n",
    "\n",
    "if reclustering=='greedy Louvain':\n",
    "    v1_df,v2_df=analysis_func.load_results_to_dfs(pc.TEMP_DIR)\n",
    "    scores_v1={'version 1':Louvain.make_partition_and_score(v1_df,test_random_graph=False,pass_weights=True,verbose=False)}\n",
    "    display(pd.DataFrame.from_dict(scores_v1,orient='index'))\n",
    "    scores_v2={'version 2':Louvain.make_partition_and_score(v2_df,test_random_graph=False,pass_weights=True,verbose=False)}\n",
    "    display(pd.DataFrame.from_dict(scores_v2,orient='index'))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   p-value  Z-score name1 name2        object1        object2\n",
      "0    0.434   -0.165  Jill   Flo  2289691956056  2289691761128\n",
      "1    0.306   -0.507  Jill  Owen  2289691956056  2289691955272\n",
      "2    0.597    0.246  Jill  Jill  2289691956056  2289691956224\n",
      "3    0.574    0.186  Jill  Jill  2289691956056  2289691955888\n",
      "4    0.735    0.628  Jill   Tom  2289691956056  2289691762192\n",
      "5    0.493   -0.019  Jill  Owen  2289691956056  2289691955384\n",
      "6    0.441   -0.148  Jill  Jill  2289691956056  2289691956112\n",
      "7    0.683    0.476  Jill   Flo  2289691956056  2289691761184\n",
      "8    0.297   -0.532  Jill   Flo  2289691956056  2289691761352\n",
      "9    0.122   -1.163  Jill   Tom  2289691956056  2289691761856\n",
      "['Tom', 'Flo', 'Jill', 'Owen']\n",
      "{2289691761128: 'Flo', 2289691955272: 'Owen', 2289691956224: 'Jill', 2289691955888: 'Jill', 2289691762192: 'Tom', 2289691955384: 'Owen', 2289691956112: 'Jill', 2289691761184: 'Flo', 2289691761352: 'Flo', 2289691761856: 'Tom', 2289691956168: 'Jill', 2289691762080: 'Tom', 2289691955496: 'Owen', 2289691955328: 'Owen', 2289691762136: 'Tom', 2289691761240: 'Flo', 2289691955440: 'Owen', 2289691762024: 'Tom', 2289691761464: 'Flo'}\n"
     ]
    }
   ],
   "source": [
    "import pointwise_correlation as pc\n",
    "import pandas as pd\n",
    "import Louvain\n",
    "import testing_and_analysis_functions as analysis_func\n",
    "v1_df,v2_df=analysis_func.load_results_to_dfs(pc.TEMP_DIR)\n",
    "#v1_df=pd.read_csv(\"{0}/sigma_v1_correlations.csv\".format(pc.TEMP_DIR))\n",
    "print(v1_df.head(10))\n",
    "#scores_v1=Louvain.make_partition_and_score(v1_df,test_random_graph=False,pass_weights=True,verbose=False)\n",
    "#display(pd.DataFrame.from_dict({'v1':scores_v1},orient='index'))\n",
    "#scores_v2=Louvain.make_partition_and_score(v2_df,test_random_graph=False,pass_weights=True,verbose=False)\n",
    "#display(pd.DataFrame.from_dict({'v2':scores_v2},orient='index'))\n",
    "\n",
    "df_results=v1_df\n",
    "names=list(set(df_results['name1']).union(set(df_results['name2'])))\n",
    "print(names)\n",
    "#nodes=list(set(df_results['object1']).union(set(df_results['object2'])))\n",
    "#print(df_results.head(40))\n",
    "#print([df_results.loc[i,'object1'] for i in df_results.index[:10]])\n",
    "#name_of_node={df_results.loc[i,'object1']:df_results.loc[i,'name1'] for i in df_results.index}\n",
    "#id_test=df_results.loc[0,'object1']\n",
    "#print(id_test)\n",
    "#print(type(id_test))\n",
    "\n",
    "name_of_node={}\n",
    "for i in df_results.index:\n",
    "    name_of_node[df_results.loc[i,'object2']]=df_results.loc[i,'name2']\n",
    "    \n",
    "print(name_of_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
