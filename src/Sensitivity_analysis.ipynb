{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity analysis of population parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    \"copy_results_out_of_temp_dir=True\\n\",\n",
    "    \"# sub-directory of Results directory to copy into\\n\",\n",
    "    \"#sub_dir=\\\"Multiple_population_correlations\\\"  \\n\",\n",
    "    \"sub_dir='Systematic_correlation_results'\\n\",\n",
    "    \"#sub_dir='Test'\\n\",\n",
    "    \"interpret_as_max_values=False\\n\",\n",
    "    \"\\n\",\n",
    "    \"for length in[1000,2000,3000,4000,5000]:\\n\",\n",
    "    \"    for number_of_populations in [3,5,8,10]:\\n\",\n",
    "    \"        for size in [5,10,15,20]:\\n\",\n",
    "    \"            for event_prob in [0.0001,0.005,0.01,0.05,0.1,0.5]:\\n\",\n",
    "    \"                for noise_prob in [0.001,0.01,0.02,0.05,0.07,0.1,0.2]:\\n\",\n",
    "    \"                    for max_lag in [4,10,20,30,50,70]: # this is always interpreted as the maximum and a random selection made up to this value  \\n\",\n",
    "    \"                        for individuals_within_a_population_have_same_incidence in [False,True]:\\n\",\n",
    "    \"                            \\n\",\n",
    "    \"                            run_single_comparison(length=length,number_of_populations=number_of_populations,\\n\",\n",
    "    \"                     size=size,event_prob=event_prob,noise_prob=noise_prob,interpret_as_max_values=interpret_as_max_values,\\n\",\n",
    "    \"                     max_lag=max_lag,individuals_within_a_population_have_same_incidence=individuals_within_a_population_have_same_incidence,\\n\",\n",
    "    \"                      copy_results_out_of_temp_dir=copy_results_out_of_temp_dir,sub_dir=sub_dir,reclustering=None)\"\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising a random population None, size 1, with betas [1000.0]\n",
      "Betas for population None are [1000.0]\n",
      "Initialising a random population None, size 1, with betas [1000.0]\n",
      "Betas for population None are [1000.0]\n",
      "Initialising for key Rosa_noise\n",
      "Expected 3 beta values, but only 1 values passed.  Initialising all time series with beta parameter 10.0\n",
      "Initialising a random population Rosa_noise, size 3, with betas [10.0, 10.0, 10.0]\n",
      "Betas for population Rosa_noise are [10.0, 10.0, 10.0]\n",
      "Elapsed time: 0.0035123825073242188\n",
      "Initialising for key Rosa\n",
      "Creating lagging time series population based on single event time series\n",
      "The poisson rates for lagging population Rosa are [18 23 11]\n",
      "Initialising population Rosa based on a prior object with event beta [1000.0]\n",
      "Mean lags for population Rosa are [18 23 11]\n",
      "Now combining poisson processes Rosa with Rosa_noise\n",
      "Elapsed time: 0.005506992340087891\n",
      "Initialising for key Luke_noise\n",
      "Expected 3 beta values, but only 1 values passed.  Initialising all time series with beta parameter 10.0\n",
      "Initialising a random population Luke_noise, size 3, with betas [10.0, 10.0, 10.0]\n",
      "Betas for population Luke_noise are [10.0, 10.0, 10.0]\n",
      "Elapsed time: 0.010207414627075195\n",
      "Initialising for key Luke\n",
      "Creating lagging time series population based on single event time series\n",
      "The poisson rates for lagging population Luke are [31  2  2]\n",
      "Initialising population Luke based on a prior object with event beta [1000.0]\n",
      "Mean lags for population Luke are [31  2  2]\n",
      "Now combining poisson processes Luke with Luke_noise\n",
      "Elapsed time: 0.01374363899230957\n",
      "T is 1000\n",
      "Analysis of tweet matrix 0: 6 time series length 1000\n",
      "Analysis of tweet matrix 1: 0 time series length 1000\n",
      "Running with inferred means (version 1).  Time elapsed: 0.0009965896606445312\n",
      "1.0,2.0,3.0,4.0,5.0,Mean non-matching stats is [-0.218779423300815, 1.0564787090150698]\n",
      "Overall mean Z-score is -0.019106306647008364\n",
      "Mean matching stats is [0.2804033683337016, 0.6203723660871758]\n",
      "Running with population means (version 2). Time elapsed: 0.1463770866394043\n",
      "1.0,2.0,3.0,4.0,5.0,Mean non-matching stats is [-0.05270258258274913, 1.026689507926333]\n",
      "Overall mean Z-score is -0.017595748965340694\n",
      "Mean matching stats is [0.03506450146077187, 0.6829381551301049]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Z score mean (v1_sigma)</th>\n",
       "      <th>Z score std (v1_sigma)</th>\n",
       "      <th>Z score mean (v2_sigma)</th>\n",
       "      <th>Z score std (v2_sigma)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>All correlations</th>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.940</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matching</th>\n",
       "      <td>0.280</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not matching</th>\n",
       "      <td>-0.219</td>\n",
       "      <td>1.056</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>1.027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Z score mean (v1_sigma)  Z score std (v1_sigma)  Z score mean (v2_sigma)  Z score std (v2_sigma)\n",
       "All correlations                   -0.019                   0.940                   -0.018                   0.906\n",
       "matching                            0.280                   0.620                    0.035                   0.683\n",
       "not matching                       -0.219                   1.056                   -0.053                   1.027"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length of time series</th>\n",
       "      <th>number of time series compared</th>\n",
       "      <th>incidence mean</th>\n",
       "      <th>incidence std</th>\n",
       "      <th>expected noise:event ratio</th>\n",
       "      <th>max lag</th>\n",
       "      <th>number of populations</th>\n",
       "      <th>individuals within populations similar</th>\n",
       "      <th>individuals across populations similar</th>\n",
       "      <th>Z score mean (v1_sigma)</th>\n",
       "      <th>Z score std (v1_sigma)</th>\n",
       "      <th>Z score mean (v2_sigma)</th>\n",
       "      <th>Z score std (v2_sigma)</th>\n",
       "      <th>raw data directory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.008</td>\n",
       "      <td>100.0</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.140</td>\n",
       "      <td>1.036</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.711</td>\n",
       "      <td>C:/Users/owen/Machine learning projects/Luc_tw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.011</td>\n",
       "      <td>100.0</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.121</td>\n",
       "      <td>1.072</td>\n",
       "      <td>0.020</td>\n",
       "      <td>1.105</td>\n",
       "      <td>C:/Users/owen/Machine learning projects/Luc_tw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.785</td>\n",
       "      <td>1.081</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>0.458</td>\n",
       "      <td>C:/Users/owen/Machine learning projects/Luc_tw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.013</td>\n",
       "      <td>100.0</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.683</td>\n",
       "      <td>C:/Users/owen/Machine learning projects/Luc_tw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length of time series  number of time series compared  incidence mean  incidence std  expected noise:event ratio  max lag  number of populations  individuals within populations similar  individuals across populations similar  Z score mean (v1_sigma)  Z score std (v1_sigma)  Z score mean (v2_sigma)  Z score std (v2_sigma)                                 raw data directory\n",
       "0                   1000                              20           0.091          0.008                       100.0       60                      2                                    True                                    True                    0.140                   1.036                    0.032                   0.711  C:/Users/owen/Machine learning projects/Luc_tw...\n",
       "1                   1000                              20           0.098          0.011                       100.0       60                      2                                    True                                    True                    0.121                   1.072                    0.020                   1.105  C:/Users/owen/Machine learning projects/Luc_tw...\n",
       "2                   1000                               6           0.095          0.004                       100.0       60                      2                                    True                                    True                   -0.785                   1.081                   -0.211                   0.458  C:/Users/owen/Machine learning projects/Luc_tw...\n",
       "3                   1000                               6           0.102          0.013                       100.0       60                      2                                    True                                    True                    0.280                   0.620                    0.035                   0.683  C:/Users/owen/Machine learning projects/Luc_tw..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "copy_results_out_of_temp_dir=True\n",
    "\n",
    "\n",
    "# sub-directory of Results directory to copy into\n",
    "\n",
    "#sub_dir=\"Multiple_population_correlations\"  \n",
    "#sub_dir='Systematic_correlation_results'\n",
    "sub_dir='Test'\n",
    "\n",
    "\n",
    "\n",
    "length = 1000\n",
    "number_of_populations=2\n",
    "size=3\n",
    "event_prob=0.001\n",
    "noise_prob=0.1\n",
    "interpret_as_max_values=False\n",
    "max_lag=60   # this is always interpreted as the maximum and a random selection made up to this value\n",
    "\n",
    "for individuals_within_a_population_have_same_incidence in [True]:\n",
    "    run_single_comparison(length=length,number_of_populations=number_of_populations,\n",
    "                     size=size,event_prob=event_prob,noise_prob=noise_prob,interpret_as_max_values=interpret_as_max_values,\n",
    "                     max_lag=max_lag,individuals_within_a_population_have_same_incidence=individuals_within_a_population_have_same_incidence,\n",
    "                      copy_results_out_of_temp_dir=copy_results_out_of_temp_dir,sub_dir=sub_dir,reclustering=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import testing_and_analysis_functions as analysis_func\n",
    "import poisson_processes as pp\n",
    "import pointwise_correlation as pc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import Louvain\n",
    "plt.rcParams['figure.figsize']=[20,10]\n",
    "\n",
    "def run_single_comparison(length,number_of_populations,\n",
    "                          size,event_prob,noise_prob,interpret_as_max_values,\n",
    "                         max_lag,individuals_within_a_population_have_same_incidence,\n",
    "                        copy_results_out_of_temp_dir,sub_dir='Multiple_population_correlations',\n",
    "                         reclustering=None,verbose=False):\n",
    "    \n",
    "    if interpret_as_max_values:\n",
    "        metaparams=analysis_func.create_random_metaparams(length=length,number_of_populations=number_of_populations,\n",
    "                                                      max_size=size,max_lag=max_lag,max_event_prob=event_prob,max_noise_prob=noise_prob)\n",
    "    else:\n",
    "        metaparams=analysis_func.create_fixed_metaparams(length=length,number_of_populations=number_of_populations,\n",
    "                                                     size=size,lag=max_lag,event_prob=event_prob,noise_prob=noise_prob)\n",
    "\n",
    "    population_params=analysis_func.directly_initialise_multiple_populations(length=length,metaparams=metaparams,\n",
    "                                                       use_fixed_means=individuals_within_a_population_have_same_incidence)\n",
    "\n",
    "    #Copy metaparameters to csv file in TEMP directory\n",
    "    metaparams['interpret size,noise and event probs as maximums']={' ':interpret_as_max_values}\n",
    "    metaparams['same incidence for all members of a population']={' ':individuals_within_a_population_have_same_incidence}\n",
    "    pd.DataFrame.from_dict(metaparams,orient='index').to_csv(\"{0}\\meta_params.csv\".format(pc.TEMP_DIR))\n",
    "    # remove metaparams that are not population keys\n",
    "    metaparams.pop('interpret size,noise and event probs as maximums')\n",
    "    metaparams.pop('same incidence for all members of a population')\n",
    "    \n",
    "    # Copy individual population parameters to csv file in TEMP directory\n",
    "    df=pd.DataFrame.from_dict(population_params,orient='index')\n",
    "    df['prior process beta'] = df['prior poisson process'].apply(lambda x: None if type(x)==float else x.betas)\n",
    "    df['mean lag list']=df['lag parameters'].apply(lambda x: None if type(x)==float else x['mus'])\n",
    "    df=df.drop(['prior poisson process','lag parameters'],axis=1)\n",
    "    df.to_csv(\"{0}\\population_parameters.csv\".format(pc.TEMP_DIR))\n",
    "\n",
    "    \n",
    "    \n",
    "    #Intialise time series objects within their populations    \n",
    "    mpp = pp.mixed_poisson_populations(length,population_params,verbose=verbose)\n",
    "    #mpp.display(stats=True)\n",
    "\n",
    "    # mix populations and extract the full array of time_series_objects\n",
    "    ts_obj1=mpp.randomly_mix_populations(list(metaparams.keys()))\n",
    "    number=len(ts_obj1)   # total number of all time series in all populations        \n",
    "    ts_obj2=[]\n",
    "\n",
    "    # compare v1 and v2 sigma measures and store correlations\n",
    "    td,df=analysis_func.compare_inferred_and_known_means(xs=[length],number=number,ts_matrices=[ts_obj1,ts_obj2],\n",
    "                                                    reclustering=None)\n",
    "    \n",
    "    # show correlation results\n",
    "    display(df)\n",
    "    \n",
    "    # measure the spread of incidence across the passed time series data\n",
    "    probs=[len(ts.t_series)/ts.T for ts in td.tweet_matrices[0]]\n",
    "    size=len(td.tweet_matrices[0])\n",
    "    incidence_stats={'length of time series' : length, 'number of time series compared': size,\n",
    "                     'incidence mean' : np.mean(probs),'incidence std' : np.std(probs),'expected noise:event ratio' : noise_prob/event_prob,\n",
    "                     'max lag' : max_lag, 'number of populations' : number_of_populations,\n",
    "                    'individuals within populations similar' : individuals_within_a_population_have_same_incidence,\n",
    "                    'individuals across populations similar' : not interpret_as_max_values}\n",
    "    \n",
    "    \n",
    "    # copy results out of pc.TEMP_DIR directory into given directory\n",
    "    if copy_results_out_of_temp_dir:\n",
    "        newly_created_sub_directory=analysis_func.copy_from_temp(dest_root_dir=\"{0}/{1}\".format(pc.RESULTS_DIR,sub_dir))\n",
    "    \n",
    "        # create index entry for most recent run\n",
    "        index_df=pd.read_csv(\"{0}\".format(pc.INDEX_FILE),index_col=0)\n",
    "        corrs_dict=df.loc['matching',:].to_dict()\n",
    "        index_df=index_df.append(pd.Series({**incidence_stats,**corrs_dict,\n",
    "                                            'raw data directory' : newly_created_sub_directory},name='{0}'.format(len(index_df))),ignore_index=False)\n",
    "        display(index_df)\n",
    "        index_df.to_csv(\"{0}\".format(pc.INDEX_FILE))\n",
    "        \n",
    "        \n",
    "        #pd.DataFrame({**incidence_stats,**corrs_dict,'raw data directory' : newly_created_sub_directory},index=[0])\\\n",
    "        #.to_csv(\"{0}/correlations_index.csv\".format(pc.RESULTS_DIR))\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "    if reclustering=='greedy Louvain':\n",
    "        v1_df,v2_df=analysis_func.load_results_to_dfs(pc.TEMP_DIR)\n",
    "        scores_v1={'version 1':Louvain.make_partition_and_score(v1_df,test_random_graph=False,pass_weights=True,verbose=False)}\n",
    "        display(pd.DataFrame.from_dict(scores_v1,orient='index'))\n",
    "        scores_v2={'version 2':Louvain.make_partition_and_score(v2_df,test_random_graph=False,pass_weights=True,verbose=False)}\n",
    "        display(pd.DataFrame.from_dict(scores_v2,orient='index'))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   p-value  Z-score name1 name2        object1        object2\n",
      "0    0.737    0.633  Jill  Jill  2376392228032  2376392227864\n",
      "1    0.298   -0.532  Jill  Jill  2376392228032  2376392227976\n",
      "2    0.980    2.045  Jill   Flo  2376392228032  2376392228760\n",
      "3    0.616    0.294  Jill   Flo  2376392228032  2376392228816\n",
      "4    0.237   -0.715  Jill   Flo  2376392228032  2376392228648\n",
      "5    0.289   -0.555  Jill   Flo  2376392228032  2376392188144\n",
      "6    0.538    0.096  Jill   Flo  2376392228032  2376392228704\n",
      "7    0.331   -0.436  Jill   Flo  2376392228032  2376392227584\n",
      "8    0.691    0.500  Jill   Flo  2376392228032  2376392227528\n",
      "9    0.434   -0.166  Jill  Jill  2376392228032  2376392227808\n",
      "['Jill', 'Flo']\n",
      "Out of 190 rows, 171 objects were consistently named and 19 were not found\n"
     ]
    }
   ],
   "source": [
    "import pointwise_correlation as pc\n",
    "import pandas as pd\n",
    "import Louvain\n",
    "import testing_and_analysis_functions as analysis_func\n",
    "v1_df,v2_df=analysis_func.load_results_to_dfs(pc.TEMP_DIR)\n",
    "#v1_df=pd.read_csv(\"{0}/sigma_v1_correlations.csv\".format(pc.TEMP_DIR))\n",
    "print(v1_df.head(10))\n",
    "#scores_v1=Louvain.make_partition_and_score(v1_df,test_random_graph=False,pass_weights=True,verbose=False)\n",
    "#display(pd.DataFrame.from_dict({'v1':scores_v1},orient='index'))\n",
    "#scores_v2=Louvain.make_partition_and_score(v2_df,test_random_graph=False,pass_weights=True,verbose=False)\n",
    "#display(pd.DataFrame.from_dict({'v2':scores_v2},orient='index'))\n",
    "\n",
    "df_results=v1_df\n",
    "names=list(set(df_results['name1']).union(set(df_results['name2'])))\n",
    "print(names)\n",
    "#nodes=list(set(df_results['object1']).union(set(df_results['object2'])))\n",
    "#print(df_results.head(40))\n",
    "#print([df_results.loc[i,'object1'] for i in df_results.index[:10]])\n",
    "#name_of_node={df_results.loc[i,'object1']:df_results.loc[i,'name1'] for i in df_results.index}\n",
    "#id_test=df_results.loc[0,'object1']\n",
    "#print(id_test)\n",
    "#print(type(id_test))\n",
    "\n",
    "name_of_node={}\n",
    "for i in df_results.index:\n",
    "    name_of_node[df_results.loc[i,'object2']]=df_results.loc[i,'name2']\n",
    "\n",
    "success_count=0\n",
    "missed_object_count=0\n",
    "for i in df_results.index:\n",
    "    pop_name=name_of_node.get(df_results.loc[i,'object1'])\n",
    "    if pop_name:\n",
    "        if pop_name == df_results.loc[i,'name1']:\n",
    "            success_count+=1\n",
    "        else:\n",
    "            print(\"We have an ID problem!!!!!\")\n",
    "            print(\"row {0} inconsistent\".format(i))\n",
    "            assert(False)\n",
    "    else:\n",
    "        missed_object_count+=1\n",
    "\n",
    "print(\"Out of {0} rows, {1} objects were consistently named and {2} were not found\"\\\n",
    "      .format(len(df_results.index),success_count,missed_object_count))\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import testing_and_analysis_functions as analysis_func\n",
    "import pointwise_correlation as pc\n",
    "analysis_func.copy_from_temp(dest_root_dir=\"{0}/Multiple_population_correlations\".format(pc.RESULTS_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "import pointwise_correlation as pc\n",
    "df=pd.read_csv(\"{0}/meta_params.csv\".format(pc.TEMP_DIR),index_col=0)\n",
    "print(df.loc[df.index[0],'T'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Y  N  Maybe  Yes1  No1  Maybe1\n",
      "Corr1  1  2      3    10   20      30\n",
      "Corr2  4  5      6    40   50      60\n",
      "{'Y': 1, 'N': 2, 'Maybe': 3, 'Yes1': 10, 'No1': 20, 'Maybe1': 30}\n",
      "{'Y': 2, 'N': 4, 'Maybe': 1, 'Yes1': 10, 'No1': 20, 'Maybe1': 3, 'Yes': 3, 'No': 5}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame([[1,2,3],[4,5,6]],columns=['Yes','No','Maybe'])\n",
    "df1=pd.DataFrame([[10,20,30],[40,50,60]],columns=['Yes1','No1','Maybe1'])\n",
    "\n",
    "df.index=['Corr1','Corr2']\n",
    "df=df.rename({'Yes': 'Y', 'No' : 'N'},axis=1)\n",
    "df1.index=['Corr1','Corr2']\n",
    "\n",
    "df[['Yes1','No1','Maybe1']]=df1\n",
    "\n",
    "print(df)\n",
    "ser=df.loc['Corr1',:].to_dict()\n",
    "print(ser)\n",
    "ser1={**ser,'Yes':3,'No':5,'Y':2,'N':4,'Maybe':1,'Maybe1':3}\n",
    "d=pd.Series(ser,name= 'data')\n",
    "\n",
    "df=df.append(pd.Series({'Yes1':3,'No1':5,'Y':2,'N':4,'Maybe':1,'Maybe1':3},name= 'data'),ignore_index=False)\n",
    "\n",
    "print(ser1)\n",
    "\n",
    "#df=pd.concat([df,ser])\n",
    "#print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
