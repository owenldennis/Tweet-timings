{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity analysis of population parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    \"copy_results_out_of_temp_dir=True\\n\",\n",
    "    \"# sub-directory of Results directory to copy into\\n\",\n",
    "    \"#sub_dir=\\\"Multiple_population_correlations\\\"  \\n\",\n",
    "    \"sub_dir='Systematic_correlation_results'\\n\",\n",
    "    \"#sub_dir='Test'\\n\",\n",
    "    \"interpret_as_max_values=False\\n\",\n",
    "    \"\\n\",\n",
    "    \"for length in[1000,2000,3000,4000,5000]:\\n\",\n",
    "    \"    for number_of_populations in [3,5,8,10]:\\n\",\n",
    "    \"        for size in [5,10,15,20]:\\n\",\n",
    "    \"            for event_prob in [0.0001,0.005,0.01,0.05,0.1,0.5]:\\n\",\n",
    "    \"                for noise_prob in [0.001,0.01,0.02,0.05,0.07,0.1,0.2]:\\n\",\n",
    "    \"                    for max_lag in [4,10,20,30,50,70]: # this is always interpreted as the maximum and a random selection made up to this value  \\n\",\n",
    "    \"                        for individuals_within_a_population_have_same_incidence in [False,True]:\\n\",\n",
    "    \"                            \\n\",\n",
    "    \"                            run_single_comparison(length=length,number_of_populations=number_of_populations,\\n\",\n",
    "    \"                     size=size,event_prob=event_prob,noise_prob=noise_prob,interpret_as_max_values=interpret_as_max_values,\\n\",\n",
    "    \"                     max_lag=max_lag,individuals_within_a_population_have_same_incidence=individuals_within_a_population_have_same_incidence,\\n\",\n",
    "    \"                      copy_results_out_of_temp_dir=copy_results_out_of_temp_dir,sub_dir=sub_dir,reclustering=None)\"\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising a random population None, size 1, with betas [20.0]\n",
      "Betas for population None are [20.0]\n",
      "Initialising a random population None, size 1, with betas [20.0]\n",
      "Betas for population None are [20.0]\n",
      "Initialising a random population None, size 1, with betas [20.0]\n",
      "Betas for population None are [20.0]\n",
      "Initialising for key Jill_noise\n",
      "Expected 20 beta values, but only 1 values passed.  Initialising all time series with beta parameter 20.0\n",
      "Initialising a random population Jill_noise, size 20, with betas [20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0]\n",
      "Betas for population Jill_noise are [20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0]\n",
      "Elapsed time: 0.011970996856689453\n",
      "Initialising for key Jill\n",
      "Creating lagging time series population based on single event time series\n",
      "The poisson rates for lagging population Jill are [41 30 57 45 41 12 52 52 12 40 22 22 56 21 24 38 46 34 57  2]\n",
      "Initialising population Jill based on a prior object with event beta [20.0]\n",
      "Mean lags for population Jill are [41 30 57 45 41 12 52 52 12 40 22 22 56 21 24 38 46 34 57  2]\n",
      "Now combining poisson processes Jill with Jill_noise\n",
      "Elapsed time: 0.029924392700195312\n",
      "Initialising for key Rosa_noise\n",
      "Expected 20 beta values, but only 1 values passed.  Initialising all time series with beta parameter 20.0\n",
      "Initialising a random population Rosa_noise, size 20, with betas [20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0]\n",
      "Betas for population Rosa_noise are [20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0]\n",
      "Elapsed time: 0.04089164733886719\n",
      "Initialising for key Rosa\n",
      "Creating lagging time series population based on single event time series\n",
      "The poisson rates for lagging population Rosa are [12 27 20 31 14  2 28  7  5 24 52 36 49 20 20 19  1 46 20 31]\n",
      "Initialising population Rosa based on a prior object with event beta [20.0]\n",
      "Mean lags for population Rosa are [12 27 20 31 14  2 28  7  5 24 52 36 49 20 20 19  1 46 20 31]\n",
      "Now combining poisson processes Rosa with Rosa_noise\n",
      "Elapsed time: 0.06270694732666016\n",
      "Initialising for key Luke_noise\n",
      "Expected 20 beta values, but only 1 values passed.  Initialising all time series with beta parameter 20.0\n",
      "Initialising a random population Luke_noise, size 20, with betas [20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0]\n",
      "Betas for population Luke_noise are [20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0, 20.0]\n",
      "Elapsed time: 0.06270694732666016\n",
      "Initialising for key Luke\n",
      "Creating lagging time series population based on single event time series\n",
      "The poisson rates for lagging population Luke are [12 30 33 46 35 54 53 19 26 26 12 12 25 58 12 20  9 49 39 41]\n",
      "Initialising population Luke based on a prior object with event beta [20.0]\n",
      "Mean lags for population Luke are [12 30 33 46 35 54 53 19 26 26 12 12 25 58 12 20  9 49 39 41]\n",
      "Now combining poisson processes Luke with Luke_noise\n",
      "Elapsed time: 0.07721376419067383\n",
      "T is 1234\n",
      "Analysis of tweet matrix 0: 60 time series length 1234\n",
      "Analysis of tweet matrix 1: 0 time series length 1234\n",
      "Running with inferred means (version 1).  Time elapsed: 0.0\n",
      "1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,Mean non-matching stats is [0.7014250160665491, 0.9289340286772635]\n",
      "Overall mean Z-score is 1.0152890204741418\n",
      "Mean matching stats is [1.6760553455427587, 1.0024008287823092]\n",
      "   Within population correlation mean  Within population correlation std  Across populations correlation mean  Across populations correlation std\n",
      "0                               1.676                              1.002                                0.701                               0.929\n",
      "Running with population means (version 2). Time elapsed: 7.540296792984009\n",
      "1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,Mean non-matching stats is [0.174212850004819, 0.6908567665733274]\n",
      "Overall mean Z-score is 0.25267284119632305\n",
      "Mean matching stats is [0.417851770020542, 0.7105935540036206]\n",
      "   Within population correlation mean  Within population correlation std  Across populations correlation mean  Across populations correlation std\n",
      "0                               1.676                              1.002                                0.701                               0.929\n",
      "0                               0.418                              0.711                                0.174                               0.691\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T</th>\n",
       "      <th>Z score std dev (v1 sigma)</th>\n",
       "      <th>Z score mean (v1 sigma)</th>\n",
       "      <th>Z score std dev (v2 sigma)</th>\n",
       "      <th>Z score mean (v2 sigma)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1234</td>\n",
       "      <td>1.056</td>\n",
       "      <td>1.015</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      T  Z score std dev (v1 sigma)  Z score mean (v1 sigma)  Z score std dev (v2 sigma)  Z score mean (v2 sigma)\n",
       "0  1234                       1.056                    1.015                       0.707                    0.253"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "copy_results_out_of_temp_dir=True\n",
    "\n",
    "\n",
    "# sub-directory of Results directory to copy into\n",
    "\n",
    "#sub_dir=\"Multiple_population_correlations\"  \n",
    "#sub_dir='Systematic_correlation_results'\n",
    "sub_dir='Test'\n",
    "\n",
    "\n",
    "\n",
    "length = 1234\n",
    "number_of_populations=3\n",
    "size=20\n",
    "event_prob=0.05\n",
    "noise_prob=0.05\n",
    "interpret_as_max_values=False\n",
    "max_lag=60   # this is always interpreted as the maximum and a random selection made up to this value\n",
    "\n",
    "for individuals_within_a_population_have_same_incidence in [True]:\n",
    "    run_single_comparison(length=length,number_of_populations=number_of_populations,\n",
    "                     size=size,event_prob=event_prob,noise_prob=noise_prob,interpret_as_max_values=interpret_as_max_values,\n",
    "                     max_lag=max_lag,individuals_within_a_population_have_same_incidence=individuals_within_a_population_have_same_incidence,\n",
    "                      copy_results_out_of_temp_dir=copy_results_out_of_temp_dir,sub_dir=sub_dir,reclustering=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import testing_and_analysis_functions as analysis_func\n",
    "import poisson_processes as pp\n",
    "import pointwise_correlation as pc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import Louvain\n",
    "plt.rcParams['figure.figsize']=[20,10]\n",
    "\n",
    "def run_single_comparison(length,number_of_populations,\n",
    "                          size,event_prob,noise_prob,interpret_as_max_values,\n",
    "                         max_lag,individuals_within_a_population_have_same_incidence,\n",
    "                        copy_results_out_of_temp_dir,sub_dir='Multiple_population_correlations',\n",
    "                         reclustering=None,verbose=False):\n",
    "    \n",
    "    if interpret_as_max_values:\n",
    "        metaparams=analysis_func.create_random_metaparams(length=length,number_of_populations=number_of_populations,\n",
    "                                                      max_size=size,max_lag=max_lag,max_event_prob=event_prob,max_noise_prob=noise_prob)\n",
    "    else:\n",
    "        metaparams=analysis_func.create_fixed_metaparams(length=length,number_of_populations=number_of_populations,\n",
    "                                                     size=size,lag=max_lag,event_prob=event_prob,noise_prob=noise_prob)\n",
    "\n",
    "    population_params=analysis_func.directly_initialise_multiple_populations(length=length,metaparams=metaparams,\n",
    "                                                       use_fixed_means=individuals_within_a_population_have_same_incidence)\n",
    "\n",
    "    #Copy metaparameters to csv file in TEMP directory\n",
    "    metaparams['interpret size,noise and event probs as maximums']={' ':interpret_as_max_values}\n",
    "    metaparams['same incidence for all members of a population']={' ':individuals_within_a_population_have_same_incidence}\n",
    "    pd.DataFrame.from_dict(metaparams,orient='index').to_csv(\"{0}\\meta_params.csv\".format(pc.TEMP_DIR))\n",
    "    # remove metaparams that are not population keys\n",
    "    metaparams.pop('interpret size,noise and event probs as maximums')\n",
    "    metaparams.pop('same incidence for all members of a population')\n",
    "    \n",
    "    # Copy individual population parameters to csv file in TEMP directory\n",
    "    df=pd.DataFrame.from_dict(population_params,orient='index')\n",
    "    df['prior process beta'] = df['prior poisson process'].apply(lambda x: None if type(x)==float else x.betas)\n",
    "    df['mean lag list']=df['lag parameters'].apply(lambda x: None if type(x)==float else x['mus'])\n",
    "    df=df.drop(['prior poisson process','lag parameters'],axis=1)\n",
    "    df.to_csv(\"{0}\\population_parameters.csv\".format(pc.TEMP_DIR))\n",
    "\n",
    "    \n",
    "    \n",
    "    #Intialise time series objects within their populations    \n",
    "    mpp = pp.mixed_poisson_populations(length,population_params,verbose=verbose)\n",
    "    #mpp.display(stats=True)\n",
    "\n",
    "    # mix populations and extract the full array of time_series_objects\n",
    "    ts_obj1=mpp.randomly_mix_populations(list(metaparams.keys()))\n",
    "    number=len(ts_obj1)   # total number of all time series in all populations        \n",
    "    ts_obj2=[]\n",
    "\n",
    "    # compare v1 and v2 sigma measures and store correlations\n",
    "    td,df=analysis_func.compare_inferred_and_known_means(xs=[length],number=number,ts_matrices=[ts_obj1,ts_obj2],\n",
    "                                                    reclustering=None)\n",
    "    display(df)\n",
    "\n",
    "    # copy results out of pc.TEMP_DIR directory into given directory\n",
    "    if copy_results_out_of_temp_dir:\n",
    "        newly_created_sub_directory=analysis_func.copy_from_temp(dest_root_dir=\"{0}/{1}\".format(pc.RESULTS_DIR,sub_dir))\n",
    "    \n",
    "    # create index entry for most recent run\n",
    "    #with open(\"INDEX_FILE.csv\") as file:\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "    if reclustering=='greedy Louvain':\n",
    "        v1_df,v2_df=analysis_func.load_results_to_dfs(pc.TEMP_DIR)\n",
    "        scores_v1={'version 1':Louvain.make_partition_and_score(v1_df,test_random_graph=False,pass_weights=True,verbose=False)}\n",
    "        display(pd.DataFrame.from_dict(scores_v1,orient='index'))\n",
    "        scores_v2={'version 2':Louvain.make_partition_and_score(v2_df,test_random_graph=False,pass_weights=True,verbose=False)}\n",
    "        display(pd.DataFrame.from_dict(scores_v2,orient='index'))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   p-value  Z-score name1 name2        object1        object2\n",
      "0    0.098   -1.292  Owen   Tom  1556018621856  1555934688312\n",
      "1    0.754    0.687  Owen  Rosa  1556018621856  1555934775840\n",
      "2    0.689    0.493  Owen  Rosa  1556018621856  1555934774160\n",
      "3    0.862    1.087  Owen  Rosa  1556018621856  1555934774832\n",
      "4    0.923    1.427  Owen  Rosa  1556018621856  1555934773936\n",
      "5    0.960    1.755  Owen  Rosa  1556018621856  1555934776624\n",
      "6    0.551    0.128  Owen   Tom  1556018621856  1555934689992\n",
      "7    0.946    1.611  Owen  Rosa  1556018621856  1555934773824\n",
      "8    0.837    0.980  Owen   Tom  1556018621856  1555934690216\n",
      "9    0.766    0.726  Owen  Rosa  1556018621856  1555934776176\n",
      "['Rosa', 'Owen', 'Tom']\n",
      "{1555934688312: 'Tom', 1555934775840: 'Rosa', 1555934774160: 'Rosa', 1555934774832: 'Rosa', 1555934773936: 'Rosa', 1555934776624: 'Rosa', 1555934689992: 'Tom', 1555934773824: 'Rosa', 1555934690216: 'Tom', 1555934776176: 'Rosa', 1555934689936: 'Tom', 1556018621240: 'Owen', 1555934690048: 'Tom', 1555934690104: 'Tom', 1556018621744: 'Owen', 1556017867072: 'Tom', 1555934690160: 'Tom', 1556018621072: 'Owen', 1555934776008: 'Rosa', 1555934776064: 'Rosa', 1555934774104: 'Rosa', 1556018542520: 'Owen', 1556018620792: 'Owen', 1555934713056: 'Tom', 1556018621464: 'Owen', 1555934689152: 'Tom', 1556018621632: 'Owen', 1555934712384: 'Tom', 1555934776344: 'Rosa', 1555934774608: 'Rosa', 1555934776680: 'Rosa', 1556018543360: 'Owen', 1555934773544: 'Rosa', 1556018621128: 'Owen', 1555934712216: 'Tom', 1556017865728: 'Tom', 1555934774328: 'Rosa', 1555934776120: 'Rosa', 1555934775280: 'Rosa', 1555934690664: 'Tom', 1555934775112: 'Owen', 1555934687528: 'Tom', 1555934774664: 'Owen', 1555934690608: 'Tom', 1555934774888: 'Owen', 1556018621408: 'Owen', 1556018621576: 'Owen', 1555934689768: 'Tom', 1555934776232: 'Rosa', 1555934775168: 'Rosa', 1556018621800: 'Owen', 1555934690552: 'Tom', 1555934776512: 'Rosa', 1556018621296: 'Owen', 1555934690496: 'Tom', 1555934690328: 'Tom', 1556018621912: 'Owen', 1556018621352: 'Owen', 1556018621520: 'Owen'}\n"
     ]
    }
   ],
   "source": [
    "import pointwise_correlation as pc\n",
    "import pandas as pd\n",
    "import Louvain\n",
    "import testing_and_analysis_functions as analysis_func\n",
    "v1_df,v2_df=analysis_func.load_results_to_dfs(pc.TEMP_DIR)\n",
    "#v1_df=pd.read_csv(\"{0}/sigma_v1_correlations.csv\".format(pc.TEMP_DIR))\n",
    "print(v1_df.head(10))\n",
    "#scores_v1=Louvain.make_partition_and_score(v1_df,test_random_graph=False,pass_weights=True,verbose=False)\n",
    "#display(pd.DataFrame.from_dict({'v1':scores_v1},orient='index'))\n",
    "#scores_v2=Louvain.make_partition_and_score(v2_df,test_random_graph=False,pass_weights=True,verbose=False)\n",
    "#display(pd.DataFrame.from_dict({'v2':scores_v2},orient='index'))\n",
    "\n",
    "df_results=v1_df\n",
    "names=list(set(df_results['name1']).union(set(df_results['name2'])))\n",
    "print(names)\n",
    "#nodes=list(set(df_results['object1']).union(set(df_results['object2'])))\n",
    "#print(df_results.head(40))\n",
    "#print([df_results.loc[i,'object1'] for i in df_results.index[:10]])\n",
    "#name_of_node={df_results.loc[i,'object1']:df_results.loc[i,'name1'] for i in df_results.index}\n",
    "#id_test=df_results.loc[0,'object1']\n",
    "#print(id_test)\n",
    "#print(type(id_test))\n",
    "\n",
    "name_of_node={}\n",
    "for i in df_results.index:\n",
    "    name_of_node[df_results.loc[i,'object2']]=df_results.loc[i,'name2']\n",
    "    \n",
    "print(name_of_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import testing_and_analysis_functions as analysis_func\n",
    "import pointwise_correlation as pc\n",
    "analysis_func.copy_from_temp(dest_root_dir=\"{0}/Multiple_population_correlations\".format(pc.RESULTS_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "import pointwise_correlation as pc\n",
    "df=pd.read_csv(\"{0}/meta_params.csv\".format(pc.TEMP_DIR),index_col=0)\n",
    "print(df.loc[df.index[0],'T'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
